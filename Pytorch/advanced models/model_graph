digraph {
	graph [size="59.25,59.25"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2188191587264 [label="
 (1, 3)" fillcolor=darkolivegreen1]
	2188226847184 [label=AddmmBackward0]
	2188226847472 -> 2188226847184
	2188223225968 [label="fc.bias
 (3)" fillcolor=lightblue]
	2188223225968 -> 2188226847472
	2188226847472 [label=AccumulateGrad]
	2188226847376 -> 2188226847184
	2188226847376 [label=ViewBackward0]
	2188226847328 -> 2188226847376
	2188226847328 [label=MeanBackward1]
	2188226847616 -> 2188226847328
	2188226847616 [label=ReluBackward0]
	2188226847712 -> 2188226847616
	2188226847712 [label=AddBackward0]
	2188226847808 -> 2188226847712
	2188226847808 [label=NativeBatchNormBackward0]
	2188226847952 -> 2188226847808
	2188226847952 [label=ConvolutionBackward0]
	2188226848144 -> 2188226847952
	2188226848144 [label=ReluBackward0]
	2188226848288 -> 2188226848144
	2188226848288 [label=NativeBatchNormBackward0]
	2188226848384 -> 2188226848288
	2188226848384 [label=ConvolutionBackward0]
	2188226847760 -> 2188226848384
	2188226847760 [label=ReluBackward0]
	2188226848672 -> 2188226847760
	2188226848672 [label=AddBackward0]
	2188226848768 -> 2188226848672
	2188226848768 [label=NativeBatchNormBackward0]
	2188226848912 -> 2188226848768
	2188226848912 [label=ConvolutionBackward0]
	2188226849104 -> 2188226848912
	2188226849104 [label=ReluBackward0]
	2188226849248 -> 2188226849104
	2188226849248 [label=NativeBatchNormBackward0]
	2188226849344 -> 2188226849248
	2188226849344 [label=ConvolutionBackward0]
	2188226849536 -> 2188226849344
	2188226849536 [label=ReluBackward0]
	2188226849680 -> 2188226849536
	2188226849680 [label=AddBackward0]
	2188226849776 -> 2188226849680
	2188226849776 [label=NativeBatchNormBackward0]
	2188226849920 -> 2188226849776
	2188226849920 [label=ConvolutionBackward0]
	2188226850112 -> 2188226849920
	2188226850112 [label=ReluBackward0]
	2188226850256 -> 2188226850112
	2188226850256 [label=NativeBatchNormBackward0]
	2188226850352 -> 2188226850256
	2188226850352 [label=ConvolutionBackward0]
	2188226849728 -> 2188226850352
	2188226849728 [label=ReluBackward0]
	2188226850640 -> 2188226849728
	2188226850640 [label=AddBackward0]
	2188226850736 -> 2188226850640
	2188226850736 [label=NativeBatchNormBackward0]
	2188226850880 -> 2188226850736
	2188226850880 [label=ConvolutionBackward0]
	2188226851072 -> 2188226850880
	2188226851072 [label=ReluBackward0]
	2188226851216 -> 2188226851072
	2188226851216 [label=NativeBatchNormBackward0]
	2188226851312 -> 2188226851216
	2188226851312 [label=ConvolutionBackward0]
	2188226851504 -> 2188226851312
	2188226851504 [label=ReluBackward0]
	2188226851648 -> 2188226851504
	2188226851648 [label=AddBackward0]
	2188226851744 -> 2188226851648
	2188226851744 [label=NativeBatchNormBackward0]
	2188226851888 -> 2188226851744
	2188226851888 [label=ConvolutionBackward0]
	2188226852080 -> 2188226851888
	2188226852080 [label=ReluBackward0]
	2188226852224 -> 2188226852080
	2188226852224 [label=NativeBatchNormBackward0]
	2188226852320 -> 2188226852224
	2188226852320 [label=ConvolutionBackward0]
	2188226851696 -> 2188226852320
	2188226851696 [label=ReluBackward0]
	2188226852608 -> 2188226851696
	2188226852608 [label=AddBackward0]
	2188226852704 -> 2188226852608
	2188226852704 [label=NativeBatchNormBackward0]
	2188226852848 -> 2188226852704
	2188226852848 [label=ConvolutionBackward0]
	2188226853040 -> 2188226852848
	2188226853040 [label=ReluBackward0]
	2188226853184 -> 2188226853040
	2188226853184 [label=NativeBatchNormBackward0]
	2188226853280 -> 2188226853184
	2188226853280 [label=ConvolutionBackward0]
	2188226853472 -> 2188226853280
	2188226853472 [label=ReluBackward0]
	2188226853616 -> 2188226853472
	2188226853616 [label=AddBackward0]
	2188226853712 -> 2188226853616
	2188226853712 [label=NativeBatchNormBackward0]
	2188226853856 -> 2188226853712
	2188226853856 [label=ConvolutionBackward0]
	2188226854048 -> 2188226853856
	2188226854048 [label=ReluBackward0]
	2188226854192 -> 2188226854048
	2188226854192 [label=NativeBatchNormBackward0]
	2188226854288 -> 2188226854192
	2188226854288 [label=ConvolutionBackward0]
	2188226853664 -> 2188226854288
	2188226853664 [label=ReluBackward0]
	2188226854576 -> 2188226853664
	2188226854576 [label=AddBackward0]
	2188226854624 -> 2188226854576
	2188226854624 [label=NativeBatchNormBackward0]
	2188226854864 -> 2188226854624
	2188226854864 [label=ConvolutionBackward0]
	2188227166416 -> 2188226854864
	2188227166416 [label=ReluBackward0]
	2188227166560 -> 2188227166416
	2188227166560 [label=NativeBatchNormBackward0]
	2188227166608 -> 2188227166560
	2188227166608 [label=ConvolutionBackward0]
	2188226854384 -> 2188227166608
	2188226854384 [label=ReluBackward0]
	2188227166992 -> 2188226854384
	2188227166992 [label=NativeBatchNormBackward0]
	2188227167040 -> 2188227166992
	2188227167040 [label=ConvolutionBackward0]
	2188227167328 -> 2188227167040
	2188191669584 [label="conv1.weight
 (64, 3, 7, 7)" fillcolor=lightblue]
	2188191669584 -> 2188227167328
	2188227167328 [label=AccumulateGrad]
	2188227166800 -> 2188227166992
	2188223161312 [label="bn1.weight
 (64)" fillcolor=lightblue]
	2188223161312 -> 2188227166800
	2188227166800 [label=AccumulateGrad]
	2188227167136 -> 2188227166992
	2188223313328 [label="bn1.bias
 (64)" fillcolor=lightblue]
	2188223313328 -> 2188227167136
	2188227167136 [label=AccumulateGrad]
	2188227166896 -> 2188227166608
	2188191668304 [label="layer1.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2188191668304 -> 2188227166896
	2188227166896 [label=AccumulateGrad]
	2188227166464 -> 2188227166560
	2188191668384 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	2188191668384 -> 2188227166464
	2188227166464 [label=AccumulateGrad]
	2188227166704 -> 2188227166560
	2188191668224 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	2188191668224 -> 2188227166704
	2188227166704 [label=AccumulateGrad]
	2188227166368 -> 2188226854864
	2188191667744 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2188191667744 -> 2188227166368
	2188227166368 [label=AccumulateGrad]
	2188226854816 -> 2188226854624
	2188191667664 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	2188191667664 -> 2188226854816
	2188226854816 [label=AccumulateGrad]
	2188226854768 -> 2188226854624
	2188191667584 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	2188191667584 -> 2188226854768
	2188226854768 [label=AccumulateGrad]
	2188226854384 -> 2188226854576
	2188226854480 -> 2188226854288
	2188191667344 [label="layer1.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2188191667344 -> 2188226854480
	2188226854480 [label=AccumulateGrad]
	2188226854240 -> 2188226854192
	2188191667104 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	2188191667104 -> 2188226854240
	2188226854240 [label=AccumulateGrad]
	2188226854096 -> 2188226854192
	2188191667024 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	2188191667024 -> 2188226854096
	2188226854096 [label=AccumulateGrad]
	2188226854000 -> 2188226853856
	2188191666544 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2188191666544 -> 2188226854000
	2188226854000 [label=AccumulateGrad]
	2188226853808 -> 2188226853712
	2188191666304 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	2188191666304 -> 2188226853808
	2188226853808 [label=AccumulateGrad]
	2188226853760 -> 2188226853712
	2188191666224 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	2188191666224 -> 2188226853760
	2188226853760 [label=AccumulateGrad]
	2188226853664 -> 2188226853616
	2188226853424 -> 2188226853280
	2188191665744 [label="layer2.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	2188191665744 -> 2188226853424
	2188226853424 [label=AccumulateGrad]
	2188226853232 -> 2188226853184
	2188191665664 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	2188191665664 -> 2188226853232
	2188226853232 [label=AccumulateGrad]
	2188226853088 -> 2188226853184
	2188191665584 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	2188191665584 -> 2188226853088
	2188226853088 [label=AccumulateGrad]
	2188226852992 -> 2188226852848
	2188191663904 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2188191663904 -> 2188226852992
	2188226852992 [label=AccumulateGrad]
	2188226852800 -> 2188226852704
	2188191670944 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	2188191670944 -> 2188226852800
	2188226852800 [label=AccumulateGrad]
	2188226852752 -> 2188226852704
	2188191670864 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	2188191670864 -> 2188226852752
	2188226852752 [label=AccumulateGrad]
	2188226852656 -> 2188226852608
	2188226852656 [label=NativeBatchNormBackward0]
	2188226853376 -> 2188226852656
	2188226853376 [label=ConvolutionBackward0]
	2188226853472 -> 2188226853376
	2188226853520 -> 2188226853376
	2188191583184 [label="layer2.0.shortcut.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	2188191583184 -> 2188226853520
	2188226853520 [label=AccumulateGrad]
	2188226852944 -> 2188226852656
	2188191585184 [label="layer2.0.shortcut.1.weight
 (128)" fillcolor=lightblue]
	2188191585184 -> 2188226852944
	2188226852944 [label=AccumulateGrad]
	2188226852896 -> 2188226852656
	2188191591904 [label="layer2.0.shortcut.1.bias
 (128)" fillcolor=lightblue]
	2188191591904 -> 2188226852896
	2188226852896 [label=AccumulateGrad]
	2188226852512 -> 2188226852320
	2188191591664 [label="layer2.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2188191591664 -> 2188226852512
	2188226852512 [label=AccumulateGrad]
	2188226852272 -> 2188226852224
	2188191582864 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	2188191582864 -> 2188226852272
	2188226852272 [label=AccumulateGrad]
	2188226852128 -> 2188226852224
	2188191582784 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	2188191582784 -> 2188226852128
	2188226852128 [label=AccumulateGrad]
	2188226852032 -> 2188226851888
	2188191582464 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2188191582464 -> 2188226852032
	2188226852032 [label=AccumulateGrad]
	2188226851840 -> 2188226851744
	2188191591424 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	2188191591424 -> 2188226851840
	2188226851840 [label=AccumulateGrad]
	2188226851792 -> 2188226851744
	2188191591344 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	2188191591344 -> 2188226851792
	2188226851792 [label=AccumulateGrad]
	2188226851696 -> 2188226851648
	2188226851456 -> 2188226851312
	2188191582144 [label="layer3.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	2188191582144 -> 2188226851456
	2188226851456 [label=AccumulateGrad]
	2188226851264 -> 2188226851216
	2188191582064 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	2188191582064 -> 2188226851264
	2188226851264 [label=AccumulateGrad]
	2188226851120 -> 2188226851216
	2188191591104 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	2188191591104 -> 2188226851120
	2188226851120 [label=AccumulateGrad]
	2188226851024 -> 2188226850880
	2188191590704 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2188191590704 -> 2188226851024
	2188226851024 [label=AccumulateGrad]
	2188226850832 -> 2188226850736
	2188191581984 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	2188191581984 -> 2188226850832
	2188226850832 [label=AccumulateGrad]
	2188226850784 -> 2188226850736
	2188191581904 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	2188191581904 -> 2188226850784
	2188226850784 [label=AccumulateGrad]
	2188226850688 -> 2188226850640
	2188226850688 [label=NativeBatchNormBackward0]
	2188226851408 -> 2188226850688
	2188226851408 [label=ConvolutionBackward0]
	2188226851504 -> 2188226851408
	2188226851552 -> 2188226851408
	2188191581344 [label="layer3.0.shortcut.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	2188191581344 -> 2188226851552
	2188226851552 [label=AccumulateGrad]
	2188226850976 -> 2188226850688
	2188191590624 [label="layer3.0.shortcut.1.weight
 (256)" fillcolor=lightblue]
	2188191590624 -> 2188226850976
	2188226850976 [label=AccumulateGrad]
	2188226850928 -> 2188226850688
	2188191590544 [label="layer3.0.shortcut.1.bias
 (256)" fillcolor=lightblue]
	2188191590544 -> 2188226850928
	2188226850928 [label=AccumulateGrad]
	2188226850544 -> 2188226850352
	2188191581024 [label="layer3.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2188191581024 -> 2188226850544
	2188226850544 [label=AccumulateGrad]
	2188226850304 -> 2188226850256
	2188191580944 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	2188191580944 -> 2188226850304
	2188226850304 [label=AccumulateGrad]
	2188226850160 -> 2188226850256
	2188191580864 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	2188191580864 -> 2188226850160
	2188226850160 [label=AccumulateGrad]
	2188226850064 -> 2188226849920
	2188191590144 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2188191590144 -> 2188226850064
	2188226850064 [label=AccumulateGrad]
	2188226849872 -> 2188226849776
	2188191590064 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	2188191590064 -> 2188226849872
	2188226849872 [label=AccumulateGrad]
	2188226849824 -> 2188226849776
	2188191580544 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	2188191580544 -> 2188226849824
	2188226849824 [label=AccumulateGrad]
	2188226849728 -> 2188226849680
	2188226849488 -> 2188226849344
	2188191589904 [label="layer4.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	2188191589904 -> 2188226849488
	2188226849488 [label=AccumulateGrad]
	2188226849296 -> 2188226849248
	2188191580304 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	2188191580304 -> 2188226849296
	2188226849296 [label=AccumulateGrad]
	2188226849152 -> 2188226849248
	2188191580224 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	2188191580224 -> 2188226849152
	2188226849152 [label=AccumulateGrad]
	2188226849056 -> 2188226848912
	2188191596064 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2188191596064 -> 2188226849056
	2188226849056 [label=AccumulateGrad]
	2188226848864 -> 2188226848768
	2188191589664 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	2188191589664 -> 2188226848864
	2188226848864 [label=AccumulateGrad]
	2188226848816 -> 2188226848768
	2188191589584 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	2188191589584 -> 2188226848816
	2188226848816 [label=AccumulateGrad]
	2188226848720 -> 2188226848672
	2188226848720 [label=NativeBatchNormBackward0]
	2188226849440 -> 2188226848720
	2188226849440 [label=ConvolutionBackward0]
	2188226849536 -> 2188226849440
	2188226849584 -> 2188226849440
	2188191595744 [label="layer4.0.shortcut.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	2188191595744 -> 2188226849584
	2188226849584 [label=AccumulateGrad]
	2188226849008 -> 2188226848720
	2188191589344 [label="layer4.0.shortcut.1.weight
 (512)" fillcolor=lightblue]
	2188191589344 -> 2188226849008
	2188226849008 [label=AccumulateGrad]
	2188226848960 -> 2188226848720
	2188191589264 [label="layer4.0.shortcut.1.bias
 (512)" fillcolor=lightblue]
	2188191589264 -> 2188226848960
	2188226848960 [label=AccumulateGrad]
	2188226848576 -> 2188226848384
	2188191595424 [label="layer4.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2188191595424 -> 2188226848576
	2188226848576 [label=AccumulateGrad]
	2188226848336 -> 2188226848288
	2188191589024 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	2188191589024 -> 2188226848336
	2188226848336 [label=AccumulateGrad]
	2188226848192 -> 2188226848288
	2188191588944 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	2188191588944 -> 2188226848192
	2188226848192 [label=AccumulateGrad]
	2188226848096 -> 2188226847952
	2188191595024 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2188191595024 -> 2188226848096
	2188226848096 [label=AccumulateGrad]
	2188226847904 -> 2188226847808
	2188191594944 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	2188191594944 -> 2188226847904
	2188226847904 [label=AccumulateGrad]
	2188226847856 -> 2188226847808
	2188191588544 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	2188191588544 -> 2188226847856
	2188226847856 [label=AccumulateGrad]
	2188226847760 -> 2188226847712
	2188226847424 -> 2188226847184
	2188226847424 [label=TBackward0]
	2188226847664 -> 2188226847424
	2188223439120 [label="fc.weight
 (3, 512)" fillcolor=lightblue]
	2188223439120 -> 2188226847664
	2188226847664 [label=AccumulateGrad]
	2188226847184 -> 2188191587264
}
